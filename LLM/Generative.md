# ğŸ¤– Generative AI â€“ Koncepcje i NarzÄ™dzia

Fundamenty LLM, wzorce integracji i narzÄ™dzia dla developerÃ³w.

---

## ğŸ§  Kluczowe PojÄ™cia

### Tokeny

```
Token â‰ˆ 0.75 sÅ‚owa (angielski) / 0.5 sÅ‚owa (polski)

"Hello, world!" = 4 tokeny
"CzeÅ›Ä‡, Å›wiecie!" = 6 tokenÃ³w

Kontekst (context window) = ile tokenÃ³w model widzi naraz:
  GPT-4o:          128k tokenÃ³w  â‰ˆ ~300 stron tekstu
  Claude Sonnet:   200k tokenÃ³w  â‰ˆ ~460 stron tekstu
  Gemini 1.5 Pro:    1M tokenÃ³w  â‰ˆ caÅ‚e repozytorium

Koszt = input tokens + output tokens (rozliczenie per 1M tokenÃ³w)
```

### Temperature i Sampling

```
temperature: 0.0 â†’ deterministyczny, zawsze ta sama odpowiedÅº
temperature: 0.7 â†’ balans kreatywnoÅ›Ä‡/spÃ³jnoÅ›Ä‡ (domyÅ›lne)
temperature: 1.0 â†’ bardziej losowy, kreatywny
temperature: 2.0 â†’ chaotyczny, niezalecany

Kiedy uÅ¼ywaÄ‡ niskiej temperatury (0.0-0.3):
  - Kod, SQL, JSON
  - Pytania z jednÄ… poprawnÄ… odpowiedziÄ…
  - Klasyfikacja, ekstrakcja danych

Kiedy uÅ¼ywaÄ‡ wyÅ¼szej (0.7-1.0):
  - TreÅ›ci kreatywne, brainstorming
  - Generowanie wariantÃ³w, parafrazowanie
```

### Typy Modeli

```
Frontier models (najwiÄ™ksze, najdroÅ¼sze):
  Claude Opus, GPT-4o, Gemini 1.5 Pro
  â†’ zÅ‚oÅ¼one zadania, reasoning, dÅ‚ugi kontekst

Mid-tier (balans jakoÅ›Ä‡/cena):
  Claude Sonnet, GPT-4o-mini, Gemini 1.5 Flash
  â†’ produkcja, wiÄ™kszoÅ›Ä‡ use-casÃ³w

Fast/Cheap (najszybsze, najtaÅ„sze):
  Claude Haiku, GPT-3.5-turbo
  â†’ proste zadania, klasyfikacja, duÅ¼e wolumeny
```

---

## ğŸ—ï¸ Wzorce Integracji

### RAG â€“ Retrieval Augmented Generation

```
Problem: LLM nie zna Twoich prywatnych danych ani aktualnoÅ›ci

RozwiÄ…zanie:
  1. Chunking    â†’ podziel dokumenty na fragmenty (~500 tokenÃ³w)
  2. Embedding   â†’ zamieÅ„ fragmenty na wektory (np. text-embedding-3-small)
  3. Vector DB   â†’ zapisz w bazie wektorowej (Pinecone, Qdrant, pgvector)
  4. Query       â†’ zamieÅ„ pytanie uÅ¼ytkownika na wektor
  5. Retrieval   â†’ znajdÅº najbardziej podobne fragmenty (cosine similarity)
  6. Augment     â†’ wstrzyknij fragmenty do promptu jako kontekst
  7. Generate    â†’ LLM odpowiada majÄ…c kontekst z Twoich danych

NarzÄ™dzia: LangChain, LlamaIndex, Vercel AI SDK
```

```ts
// Prosty RAG flow (pseudokod)
async function ragQuery(question: string) {
  const queryEmbedding = await embedText(question);
  const relevantDocs = await vectorDB.search(queryEmbedding, { topK: 5 });
  const context = relevantDocs.map((d) => d.content).join("\n\n");

  return await llm.complete({
    messages: [
      {
        role: "user",
        content: `Kontekst:\n${context}\n\nPytanie: ${question}`,
      },
    ],
  });
}
```

### Function Calling / Tool Use

```ts
// Model moÅ¼e wywoÅ‚ywaÄ‡ Twoje funkcje
const tools = [
  {
    name: "get_weather",
    description: "Pobiera aktualnÄ… pogodÄ™ dla danego miasta",
    input_schema: {
      type: "object",
      properties: {
        city: { type: "string", description: "Nazwa miasta" },
      },
      required: ["city"],
    },
  },
];

// Model decyduje kiedy wywoÅ‚aÄ‡ narzÄ™dzie
// Ty wykonujesz funkcjÄ™ i zwracasz wynik do modelu
// Model generuje koÅ„cowÄ… odpowiedÅº na podstawie danych
```

### Structured Output (JSON Mode)

```ts
// Wymuszenie odpowiedzi w formacie JSON
const response = await anthropic.messages.create({
  model: "claude-sonnet-4-5",
  messages: [
    {
      role: "user",
      content: `Wyekstrahuj dane z tekstu. Odpowiedz TYLKO w JSON bez Å¼adnego tekstu:
    {
      "name": "string",
      "email": "string | null",
      "company": "string | null"
    }

    Tekst: "CzeÅ›Ä‡, jestem Jan Kowalski z firmy Acme, jan@acme.com"`,
    },
  ],
});

const data = JSON.parse(response.content[0].text);
```

---

## ğŸ”— Agentic AI â€“ Autonomiczne Agenty

```
Agent = LLM + Tools + Loop

PÄ™tla agenta:
  1. Otrzymaj zadanie
  2. Zaplanuj kolejne kroki (reasoning)
  3. Wybierz i wywoÅ‚aj narzÄ™dzie
  4. Obserwuj wynik
  5. PowtÃ³rz aÅ¼ zadanie ukoÅ„czone

Wzorce:
  ReAct    â†’ Reason + Act (myÅ›l, dziaÅ‚aj, obserwuj)
  Chain    â†’ sekwencja krokÃ³w z gÃ³ry zdefiniowana
  Router   â†’ wybierz specjalistyczny agent do zadania

Ryzyka:
  - PÄ™tle bez wyjÅ›cia (ustaw max_iterations)
  - Koszty â€“ kaÅ¼dy krok = tokeny
  - Halucynacje przy wywoÅ‚ywaniu narzÄ™dzi
```

---

## ğŸ’¬ System Prompt vs User Prompt

```
System Prompt:
  - Rola i osobowoÅ›Ä‡ modelu
  - StaÅ‚e instrukcje i ograniczenia
  - Kontekst aplikacji
  - Format odpowiedzi
  - JÄ™zyk i ton

User Prompt:
  - Konkretne pytanie/zadanie od uÅ¼ytkownika
  - Dane do przetworzenia
  - Zmienne informacje
```

```ts
const response = await anthropic.messages.create({
  system: `JesteÅ› asystentem do code review dla zespoÅ‚u TypeScript.
  Odpowiadaj zawsze po polsku.
  Oceniaj pod kÄ…tem: bÅ‚Ä™dÃ³w, wydajnoÅ›ci, czytelnoÅ›ci.
  Format: markdown z sekcjami ## BÅ‚Ä™dy ## Sugestie ## Ocena`,

  messages: [
    {
      role: "user",
      content: `ZrÃ³b review:\n\`\`\`ts\n${code}\n\`\`\``,
    },
  ],
});
```

---

## ğŸ“Š Embeddings i Wektory

```ts
// ZamieÅ„ tekst na wektor (do porÃ³wnywania semantycznego)
const embedding = await openai.embeddings.create({
  model: "text-embedding-3-small",
  input: "Jak resetowaÄ‡ hasÅ‚o?",
});
// Zwraca tablicÄ™ 1536 liczb

// Cosine similarity â€“ mierzy podobieÅ„stwo (-1 do 1)
function cosineSimilarity(a: number[], b: number[]): number {
  const dot = a.reduce((sum, ai, i) => sum + ai * b[i], 0);
  const normA = Math.sqrt(a.reduce((sum, ai) => sum + ai * ai, 0));
  const normB = Math.sqrt(b.reduce((sum, bi) => sum + bi * bi, 0));
  return dot / (normA * normB);
}

// Zastosowania:
// - Wyszukiwanie semantyczne (RAG)
// - Klasyfikacja dokumentÃ³w
// - Wykrywanie duplikatÃ³w
// - Rekomendacje
```

---

## ğŸ› ï¸ NarzÄ™dzia i SDK

```
Frameworki:
  LangChain     â€“ najpopularniejszy, Python + JS, wszystko w jednym
  LlamaIndex    â€“ RAG-focused, zarzÄ…dzanie dokumentami
  Vercel AI SDK â€“ React hooks do streamingu, integracja Next.js

Vector Databases:
  Pinecone      â€“ managed, production-ready
  Qdrant        â€“ open-source, self-hosted
  pgvector      â€“ PostgreSQL extension (masz juÅ¼ bazÄ™!)
  Weaviate      â€“ open-source, GraphQL API

Observability:
  LangSmith     â€“ tracing dla LangChain
  Helicone      â€“ proxy z analytics dla wszystkich LLM
  Langfuse      â€“ open-source, self-hosted
```

---

## ğŸ’¡ Tips

- **Streaming** â€“ uÅ¼ywaj SSE/stream zamiast czekaÄ‡ na peÅ‚nÄ… odpowiedÅº (lepszy UX)
- **Caching** â€“ identyczne prompty z tÄ… samÄ… temperaturÄ… = identyczna odpowiedÅº, moÅ¼esz cachowaÄ‡
- **Prompt versioning** â€“ traktuj prompty jak kod: wersjonuj, testuj, mierz regresje
- **Eval > Vibe check** â€“ mierz jakoÅ›Ä‡ odpowiedzi automatycznie, nie tylko rÄ™cznie
- Zawsze miej **fallback** gdy API jest niedostÄ™pne lub zwraca bÅ‚Ä…d
- **Rate limiting** po stronie aplikacji â€“ jeden user nie powinien wyczerpaÄ‡ caÅ‚ego budÅ¼etu
